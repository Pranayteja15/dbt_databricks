[0m20:35:23.961621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107eb70e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e979d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108e97b10>]}


============================== 20:35:23.963912 | 595f5372-c431-40cc-b71f-69ba6163d9d2 ==============================
[0m20:35:23.963912 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:35:23.964174 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:35:23.972493 [info ] [MainThread]: dbt version: 1.10.1
[0m20:35:23.972644 [info ] [MainThread]: python version: 3.13.3
[0m20:35:23.972751 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
[0m20:35:23.972847 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m20:35:23.974111 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:35:23.974223 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:35:23.974319 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:35:24.010367 [info ] [MainThread]: Configuration:
[0m20:35:24.010593 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m20:35:24.010708 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:35:24.010814 [info ] [MainThread]: Required dependencies:
[0m20:35:24.010978 [debug] [MainThread]: Executing "git --help"
[0m20:35:24.026627 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:35:24.027086 [debug] [MainThread]: STDERR: "b''"
[0m20:35:24.027219 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:35:24.027347 [info ] [MainThread]: Connection test skipped since no profile was found
[0m20:35:24.027473 [info ] [MainThread]: [31m1 check failed:[0m
[0m20:35:24.027568 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Could not find profile named 'fivetran_dbt'


[0m20:35:24.029361 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": false, "command_wall_clock_time": 0.09667912, "process_in_blocks": "0", "process_kernel_time": 0.119573, "process_mem_max_rss": "110968832", "process_out_blocks": "0", "process_user_time": 0.550817}
[0m20:35:24.029595 [debug] [MainThread]: Command `dbt debug` failed at 20:35:24.029552 after 0.10 seconds
[0m20:35:24.029800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108378180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109031c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d63020>]}
[0m20:35:24.029996 [debug] [MainThread]: Flushing usage events
[0m20:35:25.179307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:35:59.517924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10625b0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10723b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10723bb10>]}


============================== 20:35:59.520321 | bb431666-cc04-45cf-ae89-cab2a6fd39e5 ==============================
[0m20:35:59.520321 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:35:59.520582 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:35:59.529418 [info ] [MainThread]: dbt version: 1.10.1
[0m20:35:59.529619 [info ] [MainThread]: python version: 3.13.3
[0m20:35:59.529735 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
[0m20:35:59.529839 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m20:35:59.937959 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:35:59.938180 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:35:59.938295 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:36:00.472870 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:36:00.473126 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:36:00.473270 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:36:00.473377 [info ] [MainThread]: adapter type: databricks
[0m20:36:00.473488 [info ] [MainThread]: adapter version: 1.10.3
[0m20:36:00.513732 [info ] [MainThread]: Configuration:
[0m20:36:00.513972 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:36:00.514098 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:36:00.514204 [info ] [MainThread]: Required dependencies:
[0m20:36:00.514389 [debug] [MainThread]: Executing "git --help"
[0m20:36:00.530616 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:36:00.531097 [debug] [MainThread]: STDERR: "b''"
[0m20:36:00.531270 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:36:00.531403 [info ] [MainThread]: Connection:
[0m20:36:00.531565 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m20:36:00.531668 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m20:36:00.531777 [info ] [MainThread]:   catalog: hive_metastore
[0m20:36:00.531876 [info ] [MainThread]:   schema: fivetranpocsample
[0m20:36:00.532154 [info ] [MainThread]: Registered adapter: databricks=1.10.3
[0m20:36:00.592358 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:36:00.592679 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:36:00.592830 [debug] [MainThread]: Using databricks connection "debug"
[0m20:36:00.592944 [debug] [MainThread]: On debug: select 1 as id
[0m20:36:00.593049 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:36:58.394580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042ef0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052cf9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052cfb10>]}


============================== 20:36:58.397044 | 8d74b247-21fe-486e-abc9-64a0a3a02a7a ==============================
[0m20:36:58.397044 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:36:58.397312 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt debug --config-dir', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:36:58.405931 [info ] [MainThread]: To view your profiles.yml file, run:

open /Users/pranayteja/.dbt
[0m20:36:58.407776 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.045443166, "process_in_blocks": "0", "process_kernel_time": 0.086157, "process_mem_max_rss": "107347968", "process_out_blocks": "0", "process_user_time": 0.504885}
[0m20:36:58.408006 [debug] [MainThread]: Command `dbt debug` succeeded at 20:36:58.407955 after 0.05 seconds
[0m20:36:58.408169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105307100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105354710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051967a0>]}
[0m20:36:58.408311 [debug] [MainThread]: Flushing usage events
[0m20:36:59.438866 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m20:37:36.437894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070df0e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105479d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110547b10>]}


============================== 20:37:36.440409 | 6a3aa3e5-3291-4d81-a4d0-1ba663b069fb ==============================
[0m20:37:36.440409 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:37:36.440691 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'log_format': 'default', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:37:36.448102 [info ] [MainThread]: dbt version: 1.10.1
[0m20:37:36.448289 [info ] [MainThread]: python version: 3.13.3
[0m20:37:36.448411 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
[0m20:37:36.448512 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m20:37:36.840292 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:37:36.840575 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:37:36.840692 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:37:37.183713 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:37:37.183954 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:37:37.184067 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:37:37.184184 [info ] [MainThread]: adapter type: databricks
[0m20:37:37.184318 [info ] [MainThread]: adapter version: 1.10.3
[0m20:37:37.221847 [info ] [MainThread]: Configuration:
[0m20:37:37.222089 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:37:37.222213 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:37:37.222324 [info ] [MainThread]: Required dependencies:
[0m20:37:37.222485 [debug] [MainThread]: Executing "git --help"
[0m20:37:37.238825 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:37:37.239335 [debug] [MainThread]: STDERR: "b''"
[0m20:37:37.239487 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:37:37.239622 [info ] [MainThread]: Connection:
[0m20:37:37.239800 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m20:37:37.239915 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m20:37:37.240034 [info ] [MainThread]:   catalog: hive_metastore
[0m20:37:37.240125 [info ] [MainThread]:   schema: fivetranpocsample
[0m20:37:37.240408 [info ] [MainThread]: Registered adapter: databricks=1.10.3
[0m20:37:37.292200 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:37:37.292562 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:37:37.292711 [debug] [MainThread]: Using databricks connection "debug"
[0m20:37:37.292825 [debug] [MainThread]: On debug: select 1 as id
[0m20:37:37.292922 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:38:46.857583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042a30e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052879d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105287b10>]}


============================== 20:38:46.860143 | 2718186a-873d-4af1-99fe-9f3719f657b2 ==============================
[0m20:38:46.860143 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:38:46.860412 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt debug', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:38:46.868758 [info ] [MainThread]: dbt version: 1.10.1
[0m20:38:46.868937 [info ] [MainThread]: python version: 3.13.3
[0m20:38:46.869056 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
[0m20:38:46.869159 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m20:38:47.249570 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:38:47.249849 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:38:47.249974 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:38:47.597335 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:38:47.597606 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:38:47.597744 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:38:47.597866 [info ] [MainThread]: adapter type: databricks
[0m20:38:47.597970 [info ] [MainThread]: adapter version: 1.10.3
[0m20:38:47.635791 [info ] [MainThread]: Configuration:
[0m20:38:47.636041 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:38:47.636163 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:38:47.636269 [info ] [MainThread]: Required dependencies:
[0m20:38:47.636432 [debug] [MainThread]: Executing "git --help"
[0m20:38:47.655995 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:38:47.656539 [debug] [MainThread]: STDERR: "b''"
[0m20:38:47.656688 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:38:47.656826 [info ] [MainThread]: Connection:
[0m20:38:47.656964 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m20:38:47.657066 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m20:38:47.657162 [info ] [MainThread]:   catalog: hive_metastore
[0m20:38:47.657262 [info ] [MainThread]:   schema: fivetranpocsample
[0m20:38:47.657358 [info ] [MainThread]:   session_properties: {'spark.databricks.delta.preview.enabled': True}
[0m20:38:47.657663 [info ] [MainThread]: Registered adapter: databricks=1.10.3
[0m20:38:47.709099 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:38:47.709429 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:38:47.709570 [debug] [MainThread]: Using databricks connection "debug"
[0m20:38:47.709693 [debug] [MainThread]: On debug: select 1 as id
[0m20:38:47.709790 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:41:14.125695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1130d30e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11464b9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11464bb10>]}


============================== 20:41:14.128251 | 457092e4-1bfa-411b-ba01-a1a7784cefbc ==============================
[0m20:41:14.128251 [info ] [MainThread]: Running with dbt=1.10.1
[0m20:41:14.128519 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:41:14.136894 [info ] [MainThread]: dbt version: 1.10.1
[0m20:41:14.137068 [info ] [MainThread]: python version: 3.13.3
[0m20:41:14.137186 [info ] [MainThread]: python path: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
[0m20:41:14.137296 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit-Mach-O
[0m20:41:14.528917 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m20:41:14.529151 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m20:41:14.529265 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m20:41:14.998615 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:41:14.998850 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:41:14.998959 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:41:14.999083 [info ] [MainThread]: adapter type: databricks
[0m20:41:14.999192 [info ] [MainThread]: adapter version: 1.10.3
[0m20:41:15.037106 [info ] [MainThread]: Configuration:
[0m20:41:15.037356 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:41:15.037482 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:41:15.037600 [info ] [MainThread]: Required dependencies:
[0m20:41:15.037773 [debug] [MainThread]: Executing "git --help"
[0m20:41:15.055593 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:41:15.056085 [debug] [MainThread]: STDERR: "b''"
[0m20:41:15.056229 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:41:15.056363 [info ] [MainThread]: Connection:
[0m20:41:15.056502 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m20:41:15.056606 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m20:41:15.056707 [info ] [MainThread]:   catalog: hive_metastore
[0m20:41:15.056801 [info ] [MainThread]:   schema: fivetranpocsample
[0m20:41:15.056896 [info ] [MainThread]:   session_properties: {'spark.databricks.delta.preview.enabled': True}
[0m20:41:15.057162 [info ] [MainThread]: Registered adapter: databricks=1.10.3
[0m20:41:15.107954 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m20:41:15.108231 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:41:15.108353 [debug] [MainThread]: Using databricks connection "debug"
[0m20:41:15.108451 [debug] [MainThread]: On debug: select 1 as id
[0m20:41:15.108542 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:14.108750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10479a530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10694d960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10694d210>]}


============================== 20:47:14.113027 | b714693f-9788-4b0d-91a1-395d170c6c58 ==============================
[0m20:47:14.113027 [info ] [MainThread]: Running with dbt=1.6.18
[0m20:47:14.113300 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt debug', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:47:14.113542 [info ] [MainThread]: dbt version: 1.6.18
[0m20:47:14.113687 [info ] [MainThread]: python version: 3.10.18
[0m20:47:14.113816 [info ] [MainThread]: python path: /opt/homebrew/opt/python@3.10/bin/python3.10
[0m20:47:14.113943 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m20:47:15.458464 [error] [MainThread]: Encountered an error:
numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
[0m20:47:15.464306 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/main.py", line 441, in debug
    results = task.run()
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/task/debug.py", line 122, in run
    load_profile_status: SubtaskStatus = self._load_profile()
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/task/debug.py", line 211, in _load_profile
    profile: Profile = Profile.render(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
  File "/opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 48, in <module>
    from databricks.sql.client import (
  File "/opt/homebrew/lib/python3.10/site-packages/databricks/sql/client.py", line 3, in <module>
    import pandas
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/__init__.py", line 13, in <module>
    from pandas._libs.interval import Interval
  File "pandas/_libs/interval.pyx", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

[0m20:47:15.466036 [debug] [MainThread]: Command `dbt debug` failed at 20:47:15.465692 after 1.37 seconds
[0m20:47:15.466523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10479a530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a40910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106db1870>]}
[0m20:47:15.467007 [debug] [MainThread]: Flushing usage events
[0m20:48:08.871996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e32410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102435480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023769b0>]}


============================== 20:48:08.874312 | f881e538-17be-4a52-874e-b43fdd1b8fb1 ==============================
[0m20:48:08.874312 [info ] [MainThread]: Running with dbt=1.6.18
[0m20:48:08.874571 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m20:48:08.874823 [info ] [MainThread]: dbt version: 1.6.18
[0m20:48:08.874950 [info ] [MainThread]: python version: 3.10.18
[0m20:48:08.875071 [info ] [MainThread]: python path: /opt/homebrew/opt/python@3.10/bin/python3.10
[0m20:48:08.875191 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m20:48:20.081662 [error] [MainThread]: Encountered an error:
cannot import name 'HeaderFactory' from 'databricks.sdk.core' (/opt/homebrew/lib/python3.10/site-packages/databricks/sdk/core.py)
[0m20:48:20.084604 [error] [MainThread]: Traceback (most recent call last):
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/cli/main.py", line 441, in debug
    results = task.run()
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/task/debug.py", line 122, in run
    load_profile_status: SubtaskStatus = self._load_profile()
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/task/debug.py", line 211, in _load_profile
    profile: Profile = Profile.render(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
  File "/opt/homebrew/Cellar/python@3.10/3.10.18/Frameworks/Python.framework/Versions/3.10/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/databricks/connections.py", line 59, in <module>
    from dbt.adapters.databricks.auth import token_auth, m2m_auth
  File "/opt/homebrew/lib/python3.10/site-packages/dbt/adapters/databricks/auth.py", line 3, in <module>
    from databricks.sdk.core import CredentialsProvider, HeaderFactory, Config, credentials_provider
ImportError: cannot import name 'HeaderFactory' from 'databricks.sdk.core' (/opt/homebrew/lib/python3.10/site-packages/databricks/sdk/core.py)

[0m20:48:20.085030 [debug] [MainThread]: Command `dbt debug` failed at 20:48:20.084977 after 11.23 seconds
[0m20:48:20.085198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e32410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1142623b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a6efee0>]}
[0m20:48:20.085359 [debug] [MainThread]: Flushing usage events
[0m20:48:53.371068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107016410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bbd480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104afe9b0>]}


============================== 20:48:53.373364 | 247e1dbe-9b51-46f6-90ef-a66cb1f47458 ==============================
[0m20:48:53.373364 [info ] [MainThread]: Running with dbt=1.6.18
[0m20:48:53.373631 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt debug', 'send_anonymous_usage_stats': 'True'}
[0m20:48:53.373873 [info ] [MainThread]: dbt version: 1.6.18
[0m20:48:53.374007 [info ] [MainThread]: python version: 3.10.18
[0m20:48:53.374132 [info ] [MainThread]: python path: /opt/homebrew/opt/python@3.10/bin/python3.10
[0m20:48:53.374261 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m20:48:54.056359 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m20:48:54.056626 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m20:48:54.056784 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m20:48:54.056946 [info ] [MainThread]: adapter type: databricks
[0m20:48:54.057074 [info ] [MainThread]: adapter version: 1.6.0
[0m20:48:54.068020 [info ] [MainThread]: Configuration:
[0m20:48:54.068265 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m20:48:54.068404 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m20:48:54.068551 [info ] [MainThread]: Required dependencies:
[0m20:48:54.068794 [debug] [MainThread]: Executing "git --help"
[0m20:48:54.089435 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m20:48:54.089956 [debug] [MainThread]: STDERR: "b''"
[0m20:48:54.090140 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m20:48:54.090294 [info ] [MainThread]: Connection:
[0m20:48:54.090473 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m20:48:54.090606 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m20:48:54.090728 [info ] [MainThread]:   catalog: hive_metastore
[0m20:48:54.090848 [info ] [MainThread]:   schema: fivetranpocsample
[0m20:48:54.091093 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m20:48:54.421284 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m20:48:54.422931 [debug] [MainThread]: Using databricks connection "debug"
[0m20:48:54.423335 [debug] [MainThread]: On debug: select 1 as id
[0m20:48:54.423748 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:48:55.992407 [info ] [MainThread]: databricks-sql-connector adapter: Successfully opened session 10f4f10c-9499-4c4f-8289-d797e58b82e9
[0m20:49:07.365182 [debug] [MainThread]: SQL status: OK in 12.9399995803833 seconds
[0m20:49:07.731724 [debug] [MainThread]: On debug: Close
[0m20:49:07.732673 [info ] [MainThread]: databricks-sql-connector adapter: Closing session 10f4f10c-9499-4c4f-8289-d797e58b82e9
[0m20:49:07.995519 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m20:49:07.996752 [info ] [MainThread]: [32mAll checks passed![0m
[0m20:49:07.998150 [debug] [MainThread]: Command `dbt debug` succeeded at 20:49:07.997852 after 14.64 seconds
[0m20:49:07.999042 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m20:49:07.999965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107016410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105755180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x142f808e0>]}
[0m20:49:08.000647 [debug] [MainThread]: Flushing usage events
[0m22:23:19.985259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103fa0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100b09480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a4a9b0>]}


============================== 22:23:19.987690 | ade0bcf0-0d01-44e5-8685-c59316d71a58 ==============================
[0m22:23:19.987690 [info ] [MainThread]: Running with dbt=1.6.18
[0m22:23:19.987999 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt init fivetran_dbt', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m22:23:19.988892 [debug] [MainThread]: Starter project path: /opt/homebrew/lib/python3.10/site-packages/dbt/include/starter_project
[0m22:23:19.992114 [info ] [MainThread]: 
Your new dbt project "fivetran_dbt" was created!

For more information on how to configure the profiles.yml file,
please consult the dbt documentation here:

  https://docs.getdbt.com/docs/configure-your-profile

One more thing:

Need help? Don't hesitate to reach out to us via GitHub issues or on Slack:

  https://community.getdbt.com/

Happy modeling!

[0m22:23:19.992381 [info ] [MainThread]: Setting up your profile.
[0m09:46:57.562200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a11e2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c561150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c560340>]}


============================== 09:46:57.564415 | f87c8a62-5a90-4d91-b599-144dade20fb6 ==============================
[0m09:46:57.564415 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:46:57.564707 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt run --select employees_renamed', 'send_anonymous_usage_stats': 'True'}
[0m09:46:59.666276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f87c8a62-5a90-4d91-b599-144dade20fb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f894b0>]}
[0m09:46:59.672842 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f87c8a62-5a90-4d91-b599-144dade20fb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c2935b0>]}
[0m09:46:59.673080 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m09:46:59.706747 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m09:46:59.707533 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:46:59.707729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f87c8a62-5a90-4d91-b599-144dade20fb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c29ecb0>]}
[0m09:46:59.964787 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m09:46:59.965241 [debug] [MainThread]: Command `dbt run` failed at 09:46:59.965193 after 2.41 seconds
[0m09:46:59.965431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a11e2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c33d4e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15c33db70>]}
[0m09:46:59.965596 [debug] [MainThread]: Flushing usage events
[0m09:47:36.120102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10754a410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10503d480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104f7a9b0>]}


============================== 09:47:36.122346 | 0f0b888a-6195-402d-8f7c-e3efc28f7c3f ==============================
[0m09:47:36.122346 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:47:36.122629 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt clean', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:47:36.141551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0f0b888a-6195-402d-8f7c-e3efc28f7c3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096f16c0>]}
[0m09:47:36.142477 [debug] [MainThread]: Command `dbt clean` succeeded at 09:47:36.142425 after 0.03 seconds
[0m09:47:36.142631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10754a410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1096f13c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109769ed0>]}
[0m09:47:36.142783 [debug] [MainThread]: Flushing usage events
[0m09:47:38.250295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e64760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a15480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1008669b0>]}


============================== 09:47:38.252044 | 5110c603-5fa1-4962-8a83-1a9a68fcbf90 ==============================
[0m09:47:38.252044 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:47:38.252299 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt deps', 'send_anonymous_usage_stats': 'True'}
[0m09:47:38.270941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5110c603-5fa1-4962-8a83-1a9a68fcbf90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105861690>]}
[0m09:47:38.271345 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:47:38.271600 [debug] [MainThread]: Command `dbt deps` succeeded at 09:47:38.271534 after 0.03 seconds
[0m09:47:38.271764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e64760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105861390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058da020>]}
[0m09:47:38.271942 [debug] [MainThread]: Flushing usage events
[0m09:47:46.439909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103580430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d5090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d4280>]}


============================== 09:47:46.441445 | 3a7d19c9-a9fb-4d72-b9f7-a38f19a1dddb ==============================
[0m09:47:46.441445 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:47:46.441730 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select employees_renamed', 'send_anonymous_usage_stats': 'True'}
[0m09:47:47.022146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a7d19c9-a9fb-4d72-b9f7-a38f19a1dddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1010054b0>]}
[0m09:47:47.028640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a7d19c9-a9fb-4d72-b9f7-a38f19a1dddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150892050>]}
[0m09:47:47.028841 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m09:47:47.042974 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m09:47:47.043434 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:47:47.043619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3a7d19c9-a9fb-4d72-b9f7-a38f19a1dddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15089f010>]}
[0m09:47:47.296240 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m09:47:47.296774 [debug] [MainThread]: Command `dbt run` failed at 09:47:47.296698 after 0.87 seconds
[0m09:47:47.297314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103580430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x150869ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15086b430>]}
[0m09:47:47.297552 [debug] [MainThread]: Flushing usage events
[0m09:48:30.700063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a21600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd5060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bd4250>]}


============================== 09:48:30.702266 | c3ee8424-ef5d-4427-8566-32316158e583 ==============================
[0m09:48:30.702266 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:48:30.702527 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select employees_renamed', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:48:31.340233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3ee8424-ef5d-4427-8566-32316158e583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1025d14b0>]}
[0m09:48:31.347074 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3ee8424-ef5d-4427-8566-32316158e583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120e920e0>]}
[0m09:48:31.347339 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m09:48:31.361353 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m09:48:31.361925 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:48:31.362114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c3ee8424-ef5d-4427-8566-32316158e583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120e9f010>]}
[0m09:48:31.622685 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m09:48:31.623126 [debug] [MainThread]: Command `dbt run` failed at 09:48:31.623080 after 0.94 seconds
[0m09:48:31.623312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a21600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120e80910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120e83af0>]}
[0m09:48:31.623469 [debug] [MainThread]: Flushing usage events
[0m09:49:32.938094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122a0a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c91480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104b6e9b0>]}


============================== 09:49:32.940411 | 7588ef98-460c-4e63-812c-2321389ceddd ==============================
[0m09:49:32.940411 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:49:32.940665 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt clean', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:49:32.959001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7588ef98-460c-4e63-812c-2321389ceddd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115761780>]}
[0m09:49:32.960089 [debug] [MainThread]: Command `dbt clean` succeeded at 09:49:32.960033 after 0.04 seconds
[0m09:49:32.960249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1122a0a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115761480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1157d82e0>]}
[0m09:49:32.960405 [debug] [MainThread]: Flushing usage events
[0m09:49:35.087631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107130760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a71480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10494a9b0>]}


============================== 09:49:35.089535 | 9b9f8831-afef-4edd-8abd-6c323eb643c4 ==============================
[0m09:49:35.089535 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:49:35.089799 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt deps', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:49:35.110196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b9f8831-afef-4edd-8abd-6c323eb643c4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124161690>]}
[0m09:49:35.110702 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m09:49:35.110966 [debug] [MainThread]: Command `dbt deps` succeeded at 09:49:35.110899 after 0.04 seconds
[0m09:49:35.111122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107130760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124161390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1241d5ed0>]}
[0m09:49:35.111267 [debug] [MainThread]: Flushing usage events
[0m09:49:37.142801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076448b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d161000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10d1601f0>]}


============================== 09:49:37.144472 | 7310617a-7fd3-4043-b636-dc7f1bcd64cd ==============================
[0m09:49:37.144472 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:49:37.144759 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select employees_renamed', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:49:37.718692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7310617a-7fd3-4043-b636-dc7f1bcd64cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052294b0>]}
[0m09:49:37.725014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7310617a-7fd3-4043-b636-dc7f1bcd64cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x148392080>]}
[0m09:49:37.725230 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m09:49:37.738801 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m09:49:37.739275 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:49:37.739465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7310617a-7fd3-4043-b636-dc7f1bcd64cd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14839f010>]}
[0m09:49:38.006297 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m09:49:38.006754 [debug] [MainThread]: Command `dbt run` failed at 09:49:38.006705 after 0.88 seconds
[0m09:49:38.006948 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076448b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x148385600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1483846a0>]}
[0m09:49:38.007112 [debug] [MainThread]: Flushing usage events
[0m09:50:22.709681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d610c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113d602b0>]}


============================== 09:50:22.711802 | a90d5741-b5e5-4e53-b7ca-09b51c867a4b ==============================
[0m09:50:22.711802 [info ] [MainThread]: Running with dbt=1.6.18
[0m09:50:22.712057 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt run --select employees_renamed', 'send_anonymous_usage_stats': 'True'}
[0m09:50:23.310440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a90d5741-b5e5-4e53-b7ca-09b51c867a4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102b654b0>]}
[0m09:50:23.317445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a90d5741-b5e5-4e53-b7ca-09b51c867a4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a193520>]}
[0m09:50:23.317727 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m09:50:23.332168 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m09:50:23.332678 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:50:23.332870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a90d5741-b5e5-4e53-b7ca-09b51c867a4b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a19edd0>]}
[0m09:50:23.591922 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m09:50:23.592362 [debug] [MainThread]: Command `dbt run` failed at 09:50:23.592314 after 0.90 seconds
[0m09:50:23.592542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055a0910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a184c70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a23cd90>]}
[0m09:50:23.592703 [debug] [MainThread]: Flushing usage events
[0m10:34:55.162494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba0430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c61090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121c60280>]}


============================== 10:34:55.164646 | 2acf9e36-d4c7-44bb-888a-124dc8a70922 ==============================
[0m10:34:55.164646 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:34:55.164921 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt run --select employees_renamed', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:34:55.821235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2acf9e36-d4c7-44bb-888a-124dc8a70922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1004bd4b0>]}
[0m10:34:55.827811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2acf9e36-d4c7-44bb-888a-124dc8a70922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15fa92050>]}
[0m10:34:55.828046 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m10:34:55.841832 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m10:34:55.842297 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:34:55.842479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2acf9e36-d4c7-44bb-888a-124dc8a70922', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15fa9f010>]}
[0m10:34:56.101718 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m10:34:56.102205 [debug] [MainThread]: Command `dbt run` failed at 10:34:56.102153 after 0.95 seconds
[0m10:34:56.102403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ba0430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15fa69ba0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15fa6b430>]}
[0m10:34:56.102581 [debug] [MainThread]: Flushing usage events
[0m10:35:17.626666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b305b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e61150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112e60340>]}


============================== 10:35:17.628701 | 38e70810-8ed3-4e6a-bd17-d3ff19954466 ==============================
[0m10:35:17.628701 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:35:17.628958 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt run --select employees_renamed', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m10:35:18.208878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '38e70810-8ed3-4e6a-bd17-d3ff19954466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e1d4b0>]}
[0m10:35:18.215622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '38e70810-8ed3-4e6a-bd17-d3ff19954466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fd935b0>]}
[0m10:35:18.215885 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m10:35:18.228041 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m10:35:18.228589 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:35:18.228962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '38e70810-8ed3-4e6a-bd17-d3ff19954466', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fd9edd0>]}
[0m10:35:18.487193 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m10:35:18.487657 [debug] [MainThread]: Command `dbt run` failed at 10:35:18.487609 after 0.87 seconds
[0m10:35:18.487858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b305b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fe3d4e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12fe3db70>]}
[0m10:35:18.488029 [debug] [MainThread]: Flushing usage events
[0m10:35:41.393585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085a0a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa610f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa602e0>]}


============================== 10:35:41.395668 | de7eda7c-c11b-4b6e-ab72-e5c04a9be02c ==============================
[0m10:35:41.395668 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:35:41.395954 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt run --select employees_renamed', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:35:41.992901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de7eda7c-c11b-4b6e-ab72-e5c04a9be02c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044154b0>]}
[0m10:35:42.000126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de7eda7c-c11b-4b6e-ab72-e5c04a9be02c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e8925f0>]}
[0m10:35:42.000421 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m10:35:42.013862 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m10:35:42.014299 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:35:42.014489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'de7eda7c-c11b-4b6e-ab72-e5c04a9be02c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e89f010>]}
[0m10:35:42.269698 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m10:35:42.270165 [debug] [MainThread]: Command `dbt run` failed at 10:35:42.270116 after 0.89 seconds
[0m10:35:42.270350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1085a0a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e8813f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13e891210>]}
[0m10:35:42.270528 [debug] [MainThread]: Flushing usage events
[0m10:36:57.434509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118232410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af61030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11af60220>]}


============================== 10:36:57.437269 | 47a55b90-2fe0-4836-9ec6-894711260262 ==============================
[0m10:36:57.437269 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:36:57.437552 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select employees_renamed', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:36:58.026976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47a55b90-2fe0-4836-9ec6-894711260262', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048654b0>]}
[0m10:36:58.034045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47a55b90-2fe0-4836-9ec6-894711260262', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1382920b0>]}
[0m10:36:58.034393 [info ] [MainThread]: Registered adapter: databricks=1.6.0
[0m10:36:58.047437 [debug] [MainThread]: checksum: 7b9fab86a08e9e96a48a825af743c566d95b877e068b8c6d0dbafbf714447189, vars: {}, profile: , target: , version: 1.6.18
[0m10:36:58.047905 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:36:58.048094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '47a55b90-2fe0-4836-9ec6-894711260262', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13829f010>]}
[0m10:36:58.359367 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found two macros named "materialization_materialized_view_default" in the
  project "dbt".
   To fix this error, rename or remove one of the following macros:
      - macros/materializations/models/materialized_view/materialized_view.sql
      - macros/materializations/models/materialized_view.sql
[0m10:36:58.360363 [debug] [MainThread]: Command `dbt run` failed at 10:36:58.360271 after 0.95 seconds
[0m10:36:58.360622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118232410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138282a70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x138282d10>]}
[0m10:36:58.360842 [debug] [MainThread]: Flushing usage events
[0m10:44:17.119452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107261c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107252110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107263690>]}


============================== 10:44:17.122296 | 5561c8d1-a7a4-4314-867f-168b8dd1e99d ==============================
[0m10:44:17.122296 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:44:17.122581 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt run --select employees_renamed', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:44:18.461399 [error] [MainThread]: Encountered an error:
numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
[0m10:44:18.470778 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 140, in wrapper
    profile = load_profile(flags.PROJECT_DIR, flags.VARS, flags.PROFILE, flags.TARGET, threads)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/runtime.py", line 70, in load_profile
    profile = Profile.render(
              ^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/connections.py", line 48, in <module>
    from databricks.sql.client import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/databricks/sql/client.py", line 3, in <module>
    import pandas
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/_libs/__init__.py", line 13, in <module>
    from pandas._libs.interval import Interval
  File "pandas/_libs/interval.pyx", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

[0m10:44:18.472334 [debug] [MainThread]: Command `dbt run` failed at 10:44:18.472150 after 1.37 seconds
[0m10:44:18.472989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074f78d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107261dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10730c390>]}
[0m10:44:18.473559 [debug] [MainThread]: Flushing usage events
[0m10:45:55.311815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105434290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a16cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1054bfed0>]}


============================== 10:45:55.314327 | 648bd742-184b-4592-8133-189d0c2da9e3 ==============================
[0m10:45:55.314327 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:45:55.314599 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:45:55.314843 [info ] [MainThread]: dbt version: 1.6.18
[0m10:45:55.314989 [info ] [MainThread]: python version: 3.11.8
[0m10:45:55.315126 [info ] [MainThread]: python path: /Users/pranayteja/.pyenv/versions/3.11.8/bin/python3.11
[0m10:45:55.315251 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m10:45:55.316810 [info ] [MainThread]: Using profiles dir at /Users/pranayteja/.dbt
[0m10:45:55.316948 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m10:45:55.317069 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m10:45:55.327459 [info ] [MainThread]: Configuration:
[0m10:45:55.327701 [info ] [MainThread]:   profiles.yml file [[31mERROR invalid[0m]
[0m10:45:55.327833 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m10:45:55.327957 [info ] [MainThread]: Required dependencies:
[0m10:45:55.328131 [debug] [MainThread]: Executing "git --help"
[0m10:45:55.340570 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m10:45:55.341075 [debug] [MainThread]: STDERR: "b''"
[0m10:45:55.341438 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m10:45:55.341656 [info ] [MainThread]: Connection test skipped since no profile was found
[0m10:45:55.341844 [info ] [MainThread]: [31m1 check failed:[0m
[0m10:45:55.341983 [info ] [MainThread]: Profile loading failed for the following reason:
Runtime Error
  Could not find profile named 'fivetran_transformations'


[0m10:45:55.342369 [debug] [MainThread]: Command `dbt debug` failed at 10:45:55.342292 after 0.05 seconds
[0m10:45:55.342633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105400710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105400610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1039a9690>]}
[0m10:45:55.342866 [debug] [MainThread]: Flushing usage events
[0m10:46:13.191892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12662df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125fcd5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125f651d0>]}


============================== 10:46:13.194372 | c276e072-c299-4871-8a1d-aae14a2417b2 ==============================
[0m10:46:13.194372 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:46:13.194648 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:46:13.194882 [info ] [MainThread]: dbt version: 1.6.18
[0m10:46:13.195029 [info ] [MainThread]: python version: 3.11.8
[0m10:46:13.195163 [info ] [MainThread]: python path: /Users/pranayteja/.pyenv/versions/3.11.8/bin/python3.11
[0m10:46:13.195292 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m10:46:13.291760 [error] [MainThread]: Encountered an error:
numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
[0m10:46:13.296636 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/main.py", line 441, in debug
    results = task.run()
              ^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 122, in run
    load_profile_status: SubtaskStatus = self._load_profile()
                                         ^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 211, in _load_profile
    profile: Profile = Profile.render(
                       ^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/connections.py", line 48, in <module>
    from databricks.sql.client import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/databricks/sql/client.py", line 3, in <module>
    import pandas
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/_libs/__init__.py", line 13, in <module>
    from pandas._libs.interval import Interval
  File "pandas/_libs/interval.pyx", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

[0m10:46:13.297406 [debug] [MainThread]: Command `dbt debug` failed at 10:46:13.297333 after 0.12 seconds
[0m10:46:13.297597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1261f3310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1262f0590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x125f1d190>]}
[0m10:46:13.297779 [debug] [MainThread]: Flushing usage events
[0m10:47:38.245590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10570bb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064e0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063f3bd0>]}


============================== 10:47:38.247968 | 10b6478c-0995-49ec-90ae-55311c42ccfe ==============================
[0m10:47:38.247968 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:47:38.248236 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt debug', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:47:38.248465 [info ] [MainThread]: dbt version: 1.6.18
[0m10:47:38.248612 [info ] [MainThread]: python version: 3.11.8
[0m10:47:38.248745 [info ] [MainThread]: python path: /Users/pranayteja/.pyenv/versions/3.11.8/bin/python3.11
[0m10:47:38.248869 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m10:47:39.576786 [error] [MainThread]: Encountered an error:
numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject
[0m10:47:39.580728 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/main.py", line 441, in debug
    results = task.run()
              ^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 122, in run
    load_profile_status: SubtaskStatus = self._load_profile()
                                         ^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 211, in _load_profile
    profile: Profile = Profile.render(
                       ^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/connections.py", line 48, in <module>
    from databricks.sql.client import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/databricks/sql/client.py", line 3, in <module>
    import pandas
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/__init__.py", line 18, in <module>
    from pandas.compat.numpy import (
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py", line 4, in <module>
    from pandas.util.version import Version
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/__init__.py", line 2, in <module>
    from pandas.util._decorators import (  # noqa:F401
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/util/_decorators.py", line 14, in <module>
    from pandas._libs.properties import cache_readonly
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/pandas/_libs/__init__.py", line 13, in <module>
    from pandas._libs.interval import Interval
  File "pandas/_libs/interval.pyx", line 1, in init pandas._libs.interval
ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject

[0m10:47:39.582153 [debug] [MainThread]: Command `dbt debug` failed at 10:47:39.581999 after 1.35 seconds
[0m10:47:39.582543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106510b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a3cfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100fbde10>]}
[0m10:47:39.582928 [debug] [MainThread]: Flushing usage events
[0m10:48:11.647966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c61b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c48790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113c481d0>]}


============================== 10:48:11.650457 | 36a219f3-ac3b-46fe-94f1-7af6fbe8aa95 ==============================
[0m10:48:11.650457 [info ] [MainThread]: Running with dbt=1.6.18
[0m10:48:11.650722 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt debug', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:48:11.650960 [info ] [MainThread]: dbt version: 1.6.18
[0m10:48:11.651102 [info ] [MainThread]: python version: 3.11.8
[0m10:48:11.651237 [info ] [MainThread]: python path: /Users/pranayteja/.pyenv/versions/3.11.8/bin/python3.11
[0m10:48:11.651362 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m10:48:21.276095 [error] [MainThread]: Encountered an error:
cannot import name 'HeaderFactory' from 'databricks.sdk.core' (/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/databricks/sdk/core.py)
[0m10:48:21.279675 [error] [MainThread]: Traceback (most recent call last):
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 87, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/requires.py", line 72, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/cli/main.py", line 441, in debug
    results = task.run()
              ^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 122, in run
    load_profile_status: SubtaskStatus = self._load_profile()
                                         ^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/task/debug.py", line 211, in _load_profile
    profile: Profile = Profile.render(
                       ^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 399, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 365, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 321, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/config/profile.py", line 145, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/factory.py", line 58, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/__init__.py", line 1, in <module>
    from dbt.adapters.databricks.connections import DatabricksConnectionManager  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/connections.py", line 59, in <module>
    from dbt.adapters.databricks.auth import token_auth, m2m_auth
  File "/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/dbt/adapters/databricks/auth.py", line 3, in <module>
    from databricks.sdk.core import CredentialsProvider, HeaderFactory, Config, credentials_provider
ImportError: cannot import name 'HeaderFactory' from 'databricks.sdk.core' (/Users/pranayteja/.pyenv/versions/3.11.8/lib/python3.11/site-packages/databricks/sdk/core.py)

[0m10:48:21.280238 [debug] [MainThread]: Command `dbt debug` failed at 10:48:21.280180 after 9.64 seconds
[0m10:48:21.280416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114229410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113cea2d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x113eabb90>]}
[0m10:48:21.280587 [debug] [MainThread]: Flushing usage events
[0m10:49:13.392708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a67a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050b3790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049aae10>]}


============================== 10:49:13.395159 | 2e268fb6-1ce4-472c-beab-16c831710a36 ==============================
[0m10:49:13.395159 [info ] [MainThread]: Running with dbt=1.5.11
[0m10:49:13.395426 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:49:14.075680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049dc410>]}
[0m10:49:14.082917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f2bb190>]}
[0m10:49:14.083159 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m10:49:14.356825 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m10:49:14.357548 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m10:49:14.357900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f2c9590>]}
[0m10:49:14.783967 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m10:49:14.815736 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.fivetran_dbt.example
[0m10:49:14.817946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f394f90>]}
[0m10:49:14.822138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f476c10>]}
[0m10:49:14.822317 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m10:49:14.822491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1022a0350>]}
[0m10:49:14.822918 [info ] [MainThread]: 
[0m10:49:14.823217 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:49:14.823593 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m10:49:14.823733 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m10:49:14.823823 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m10:49:14.823905 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:51:42.783228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112211310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1123ba8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11281f350>]}


============================== 10:51:42.785597 | ae447cb6-5743-402b-bdd6-e42a3725f277 ==============================
[0m10:51:42.785597 [info ] [MainThread]: Running with dbt=1.5.11
[0m10:51:42.785870 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m10:51:42.786096 [warn ] [MainThread]: [OpenCommand]: Unable to parse dict {'open_cmd': 'open', 'profiles_dir': PosixPath('/Users/pranayteja/.dbt')}
[0m10:51:42.786240 [info ] [MainThread]: To view your profiles.yml file, run:

 
[0m10:51:42.786476 [debug] [MainThread]: Command `dbt debug` succeeded at 10:51:42.786423 after 0.02 seconds
[0m10:51:42.786611 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112212690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1124c3c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bfa550>]}
[0m10:51:42.786751 [debug] [MainThread]: Flushing usage events
[0m10:56:36.775807 [debug] [ThreadPool]: SQL status: OK in 441.95001220703125 seconds
[0m10:56:36.786936 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m10:56:38.083357 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m10:56:38.084655 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m10:56:38.096736 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m10:56:38.097124 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m10:56:38.097408 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m10:56:38.097684 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:56:54.074224 [debug] [ThreadPool]: SQL status: OK in 15.979999542236328 seconds
[0m10:56:55.080051 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m10:56:55.081382 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m10:56:55.082052 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m10:56:55.082329 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m10:56:55.970733 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m10:56:55.979404 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m10:56:55.980086 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m10:56:55.980482 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m10:56:58.314279 [debug] [ThreadPool]: SQL status: OK in 2.3299999237060547 seconds
[0m10:56:58.319154 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m10:56:59.411260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cad350>]}
[0m10:56:59.413325 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:56:59.413732 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:56:59.414627 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:56:59.415220 [info ] [MainThread]: 
[0m10:56:59.424543 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m10:56:59.425320 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m10:56:59.426529 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m10:56:59.426833 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m10:56:59.433863 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m10:56:59.434838 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 10:56:59.427035 => 10:56:59.434685
[0m10:56:59.435060 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m10:56:59.452745 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m10:56:59.453241 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m10:56:59.453372 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m10:56:59.453526 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`Employees`

[0m10:56:59.453697 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m10:57:01.842924 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`Employees`

[0m10:57:01.843493 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m10:57:01.844120 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more

[0m10:57:01.844753 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x07\xa4t\xe9/\x14Ja\xa5n\xbc8\xa6\xab\xe6B'
[0m10:57:01.845271 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 10:56:59.435200 => 10:57:01.845081
[0m10:57:01.845619 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m10:57:01.845925 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m10:57:01.846478 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m10:57:02.727943 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m10:57:02.728721 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2e268fb6-1ce4-472c-beab-16c831710a36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13f2ecb90>]}
[0m10:57:02.729334 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 3.30s]
[0m10:57:02.729840 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m10:57:02.731397 [debug] [MainThread]: On master: ROLLBACK
[0m10:57:02.731719 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:57:03.619382 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:57:03.620862 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m10:57:03.621767 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m10:57:03.622290 [debug] [MainThread]: On master: ROLLBACK
[0m10:57:03.622746 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m10:57:03.623169 [debug] [MainThread]: On master: Close
[0m10:57:04.518420 [debug] [MainThread]: Connection 'master' was properly closed.
[0m10:57:04.519374 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m10:57:04.522124 [info ] [MainThread]: 
[0m10:57:04.522952 [info ] [MainThread]: Finished running 1 view model in 0 hours 7 minutes and 49.70 seconds (469.70s).
[0m10:57:04.524274 [debug] [MainThread]: Command end result
[0m10:57:04.537263 [info ] [MainThread]: 
[0m10:57:04.537765 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m10:57:04.538074 [info ] [MainThread]: 
[0m10:57:04.538381 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m10:57:04.538662 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m10:57:04.538931 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m10:57:04.539199 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m10:57:04.539471 [info ] [MainThread]: 
[0m10:57:04.539804 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m10:57:04.540421 [debug] [MainThread]: Command `dbt run` failed at 10:57:04.540329 after 471.16 seconds
[0m10:57:04.540878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c84150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1050aa4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102a12610>]}
[0m10:57:04.541196 [debug] [MainThread]: Flushing usage events
[0m11:02:12.416205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d42c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049d9b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049b9250>]}


============================== 11:02:12.418682 | c7b8cef8-e826-4b6e-8147-3a076eceb567 ==============================
[0m11:02:12.418682 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:02:12.418953 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:02:12.430227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c7b8cef8-e826-4b6e-8147-3a076eceb567', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fcfe90>]}
[0m11:02:12.433148 [debug] [MainThread]: Command `dbt clean` succeeded at 11:02:12.433097 after 0.03 seconds
[0m11:02:12.433292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d3bd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a64450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104fd4210>]}
[0m11:02:12.433436 [debug] [MainThread]: Flushing usage events
[0m11:02:28.132655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107af0390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108529210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10852b7d0>]}


============================== 11:02:28.135237 | 048d29df-a0f0-4248-a8ba-ba668f1bdf91 ==============================
[0m11:02:28.135237 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:02:28.135529 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'profiles_dir': '/Users/pranayteja/.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:02:28.135754 [info ] [MainThread]: dbt version: 1.5.11
[0m11:02:28.135904 [info ] [MainThread]: python version: 3.11.8
[0m11:02:28.136053 [info ] [MainThread]: python path: /Users/pranayteja/.pyenv/versions/3.11.8/bin/python3.11
[0m11:02:28.136186 [info ] [MainThread]: os info: macOS-15.5-arm64-arm-64bit
[0m11:02:28.136314 [info ] [MainThread]: Using profiles.yml file at /Users/pranayteja/.dbt/profiles.yml
[0m11:02:28.136451 [info ] [MainThread]: Using dbt_project.yml file at /Users/pranayteja/fivetran_transformations/fivetran_dbt/dbt_project.yml
[0m11:02:28.136569 [info ] [MainThread]: Configuration:
[0m11:02:28.819712 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m11:02:28.830469 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m11:02:28.830757 [info ] [MainThread]: Required dependencies:
[0m11:02:28.830958 [debug] [MainThread]: Executing "git --help"
[0m11:02:28.846959 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m11:02:28.847367 [debug] [MainThread]: STDERR: "b''"
[0m11:02:28.847491 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m11:02:28.847640 [info ] [MainThread]: Connection:
[0m11:02:28.847827 [info ] [MainThread]:   host: adb-1467453450897574.14.azuredatabricks.net
[0m11:02:28.847947 [info ] [MainThread]:   http_path: sql/protocolv1/o/1467453450897574/0617-091726-uh57x7i
[0m11:02:28.848065 [info ] [MainThread]:   catalog: hive_metastore
[0m11:02:28.848174 [info ] [MainThread]:   schema: fivetranpocsample
[0m11:02:28.848425 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:02:28.853318 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m11:02:28.853938 [debug] [MainThread]: Using databricks connection "debug"
[0m11:02:28.854126 [debug] [MainThread]: On debug: select 1 as id
[0m11:02:28.854270 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:02:34.384604 [debug] [MainThread]: SQL status: OK in 5.53000020980835 seconds
[0m11:02:34.386844 [debug] [MainThread]: On debug: Close
[0m11:02:35.424434 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m11:02:35.425871 [info ] [MainThread]: [32mAll checks passed![0m
[0m11:02:35.428177 [debug] [MainThread]: Command `dbt debug` succeeded at 11:02:35.427906 after 7.31 seconds
[0m11:02:35.428909 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m11:02:35.429960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082e2f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x141161590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f40310>]}
[0m11:02:35.430786 [debug] [MainThread]: Flushing usage events
[0m11:02:47.396384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10470bbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044ca7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447f910>]}


============================== 11:02:47.398485 | fffdbc7a-e33e-4d05-9e96-a8add6a1fc82 ==============================
[0m11:02:47.398485 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:02:47.398771 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:02:47.919377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447f750>]}
[0m11:02:47.926455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10447f710>]}
[0m11:02:47.926684 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:02:47.942336 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:02:47.942798 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:02:47.942988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13edfb190>]}
[0m11:02:48.372958 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:02:48.406004 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.fivetran_dbt.example
[0m11:02:48.408373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13ee12710>]}
[0m11:02:48.411554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13ed7e510>]}
[0m11:02:48.411736 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:02:48.411886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101db7e50>]}
[0m11:02:48.412387 [info ] [MainThread]: 
[0m11:02:48.412716 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:02:48.413200 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:02:48.413379 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:02:48.413488 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:02:48.413594 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:02:50.214113 [debug] [ThreadPool]: SQL status: OK in 1.7999999523162842 seconds
[0m11:02:50.220725 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:02:51.244874 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:02:51.246974 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:02:51.263794 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:02:51.264331 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:02:51.264612 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:02:51.264857 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:03:09.476060 [debug] [ThreadPool]: SQL status: OK in 18.209999084472656 seconds
[0m11:03:10.459987 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:03:10.460636 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:03:10.460963 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:03:10.461228 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:03:11.343619 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:03:11.354523 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:03:11.355158 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:03:11.355486 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:03:15.417069 [debug] [ThreadPool]: SQL status: OK in 4.059999942779541 seconds
[0m11:03:15.422143 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:03:16.338503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c8cdd0>]}
[0m11:03:16.339925 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:16.340364 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:03:16.341411 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:03:16.341934 [info ] [MainThread]: 
[0m11:03:16.349493 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:03:16.350484 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:03:16.351898 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:03:16.352324 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:03:16.361440 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:03:16.362866 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:03:16.352550 => 11:03:16.362646
[0m11:03:16.363215 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:03:16.384015 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:03:16.385249 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:16.385409 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:03:16.385574 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`Employees`

[0m11:03:16.385762 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:03:18.270210 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`Employees`

[0m11:03:18.274031 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:03:18.274634 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more

[0m11:03:18.275212 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x92_z\xc8\xdd\x07C\xd3\x92|\xad\xacz\xbdC\xc4'
[0m11:03:18.276178 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:03:16.363423 => 11:03:18.275793
[0m11:03:18.276754 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:03:18.277052 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:03:18.277327 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:03:19.198396 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:03:19.201471 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fffdbc7a-e33e-4d05-9e96-a8add6a1fc82', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x13edc9a90>]}
[0m11:03:19.202266 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 2.85s]
[0m11:03:19.202609 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:03:19.204311 [debug] [MainThread]: On master: ROLLBACK
[0m11:03:19.204653 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:03:20.123541 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:03:20.123909 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:03:20.124050 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:03:20.124182 [debug] [MainThread]: On master: ROLLBACK
[0m11:03:20.124301 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:03:20.124412 [debug] [MainThread]: On master: Close
[0m11:03:21.004421 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:03:21.005151 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:03:21.007291 [info ] [MainThread]: 
[0m11:03:21.007707 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 32.59 seconds (32.59s).
[0m11:03:21.008503 [debug] [MainThread]: Command end result
[0m11:03:21.022260 [info ] [MainThread]: 
[0m11:03:21.022929 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:03:21.023266 [info ] [MainThread]: 
[0m11:03:21.023472 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m11:03:21.023653 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`Employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m11:03:21.024033 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m11:03:21.024197 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:03:21.024422 [info ] [MainThread]: 
[0m11:03:21.024663 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:03:21.025178 [debug] [MainThread]: Command `dbt run` failed at 11:03:21.025115 after 33.64 seconds
[0m11:03:21.025482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104714b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104405f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1040e5c10>]}
[0m11:03:21.025661 [debug] [MainThread]: Flushing usage events
[0m11:05:42.746866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1207d1f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ead990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120eac8d0>]}


============================== 11:05:42.749350 | 57ea06b7-a944-4710-8f57-465eb5e52a31 ==============================
[0m11:05:42.749350 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:05:42.749650 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:05:43.444168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ec5510>]}
[0m11:05:43.451131 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120ec5510>]}
[0m11:05:43.451368 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:05:43.464258 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:05:43.470805 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m11:05:43.471133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159debe10>]}
[0m11:05:43.878846 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:05:43.913816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159bbc5d0>]}
[0m11:05:43.917270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159eb7650>]}
[0m11:05:43.917471 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:05:43.917628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042f4110>]}
[0m11:05:43.918124 [info ] [MainThread]: 
[0m11:05:43.918455 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:05:43.918867 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:05:43.919019 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:05:43.919109 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:05:43.919197 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:05:45.787872 [debug] [ThreadPool]: SQL status: OK in 1.8700000047683716 seconds
[0m11:05:45.793828 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:05:46.671839 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:05:46.672738 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:05:46.683940 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:46.684204 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:05:46.684392 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:05:46.684568 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:05:48.855080 [debug] [ThreadPool]: SQL status: OK in 2.1700000762939453 seconds
[0m11:05:48.856408 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:05:48.856727 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:05:48.857144 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:05:48.857461 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:05:49.731275 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:05:49.738522 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:05:49.738914 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:05:49.739176 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:05:51.850842 [debug] [ThreadPool]: SQL status: OK in 2.109999895095825 seconds
[0m11:05:51.854985 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:05:52.746450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159d8c290>]}
[0m11:05:52.747423 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:52.747762 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:05:52.748614 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:05:52.749192 [info ] [MainThread]: 
[0m11:05:52.754272 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:05:52.754849 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:05:52.755607 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:05:52.755920 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:05:52.764773 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:05:52.765860 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:05:52.756121 => 11:05:52.765593
[0m11:05:52.766136 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:05:52.785497 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:05:52.786191 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:52.786336 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:05:52.786486 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`employees`

[0m11:05:52.786650 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:05:54.739461 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `hive_metastore`.`fivetranpocsample`.`employees`

[0m11:05:54.740240 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:05:54.741082 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more

[0m11:05:54.741834 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'@\x88\xc4IZ\xb5L\xec\xafC\x89\x9c{\xc8\x08\xa4'
[0m11:05:54.742395 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:05:52.766279 => 11:05:54.742178
[0m11:05:54.742764 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:05:54.743084 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:05:54.743381 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:05:55.670628 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:05:55.671267 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '57ea06b7-a944-4710-8f57-465eb5e52a31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159da7c10>]}
[0m11:05:55.671783 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 2.92s]
[0m11:05:55.672206 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:05:55.673527 [debug] [MainThread]: On master: ROLLBACK
[0m11:05:55.673820 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:05:56.561175 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:05:56.562253 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:05:56.562581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:05:56.562865 [debug] [MainThread]: On master: ROLLBACK
[0m11:05:56.563132 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:05:56.563379 [debug] [MainThread]: On master: Close
[0m11:05:57.464497 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:05:57.465110 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:05:57.466957 [info ] [MainThread]: 
[0m11:05:57.467426 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 13.55 seconds (13.55s).
[0m11:05:57.468082 [debug] [MainThread]: Command end result
[0m11:05:57.477685 [info ] [MainThread]: 
[0m11:05:57.478020 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:05:57.478257 [info ] [MainThread]: 
[0m11:05:57.478495 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m11:05:57.478888 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `hive_metastore`.`fivetranpocsample`.`employees` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m11:05:57.479226 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m11:05:57.479494 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
[0m11:05:57.479741 [info ] [MainThread]: 
[0m11:05:57.480014 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:05:57.480426 [debug] [MainThread]: Command `dbt run` failed at 11:05:57.480345 after 14.74 seconds
[0m11:05:57.480706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120bad350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x120561450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x159db7e90>]}
[0m11:05:57.480966 [debug] [MainThread]: Flushing usage events
[0m11:06:33.357722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10559ea90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103ac6490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105539310>]}


============================== 11:06:33.360356 | 75049921-1c6a-4e39-a5ab-f2345ac89805 ==============================
[0m11:06:33.360356 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:06:33.360652 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:06:34.068437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052b3690>]}
[0m11:06:34.075336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14fe5e4d0>]}
[0m11:06:34.075533 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:06:34.089527 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:06:34.108097 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:06:34.108415 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/employees_renamed.sql
[0m11:06:34.118973 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:06:34.134530 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14ff00bd0>]}
[0m11:06:34.137815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x158090710>]}
[0m11:06:34.138007 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:06:34.138162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x158085450>]}
[0m11:06:34.138689 [info ] [MainThread]: 
[0m11:06:34.139023 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:06:34.139467 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:06:34.139622 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:06:34.139725 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:06:34.139818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:06:35.929342 [debug] [ThreadPool]: SQL status: OK in 1.7899999618530273 seconds
[0m11:06:35.935849 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:06:36.796705 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:06:36.797787 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:06:36.809949 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:36.810258 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:06:36.810483 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:06:36.810693 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:06:38.708193 [debug] [ThreadPool]: SQL status: OK in 1.899999976158142 seconds
[0m11:06:38.709716 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:06:38.710034 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:06:38.710291 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:06:38.710514 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:06:39.559718 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:06:39.566722 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:06:39.567049 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:06:39.567262 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:06:41.684633 [debug] [ThreadPool]: SQL status: OK in 2.119999885559082 seconds
[0m11:06:41.688420 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:06:42.564356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14fee7050>]}
[0m11:06:42.565178 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:42.565497 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:06:42.566206 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:06:42.566588 [info ] [MainThread]: 
[0m11:06:42.571004 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:06:42.571572 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:06:42.572316 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:06:42.572636 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:06:42.580627 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:06:42.581325 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:06:42.574365 => 11:06:42.581152
[0m11:06:42.581578 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:06:42.599657 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:06:42.600192 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:42.600325 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:06:42.600484 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
  table_catalog,
  table_schema,
  table_name
FROM information_schema.tables 
WHERE table_schema = 'fivetranpocsample'

[0m11:06:42.600648 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:06:46.441666 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
  table_catalog,
  table_schema,
  table_name
FROM information_schema.tables 
WHERE table_schema = 'fivetranpocsample'

[0m11:06:46.442452 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `information_schema`.`tables` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m11:06:46.443229 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `information_schema`.`tables` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `information_schema`.`tables` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more

[0m11:06:46.443947 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xac\x9f\xf1:\x88<N\xe0\x9a\x07Rxj:C\xad'
[0m11:06:46.444506 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:06:42.581717 => 11:06:46.444292
[0m11:06:46.444893 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:06:46.445214 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:06:46.445523 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:06:47.341575 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `information_schema`.`tables` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m11:06:47.342187 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '75049921-1c6a-4e39-a5ab-f2345ac89805', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1580ffb10>]}
[0m11:06:47.342738 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 4.77s]
[0m11:06:47.343168 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:06:47.344405 [debug] [MainThread]: On master: ROLLBACK
[0m11:06:47.344683 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:06:48.229374 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:06:48.229978 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:06:48.230282 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:06:48.230563 [debug] [MainThread]: On master: ROLLBACK
[0m11:06:48.230829 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:06:48.231073 [debug] [MainThread]: On master: Close
[0m11:06:49.103529 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:06:49.104212 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:06:49.106645 [info ] [MainThread]: 
[0m11:06:49.107286 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 14.97 seconds (14.97s).
[0m11:06:49.107984 [debug] [MainThread]: Command end result
[0m11:06:49.117209 [info ] [MainThread]: 
[0m11:06:49.117966 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:06:49.118399 [info ] [MainThread]: 
[0m11:06:49.118716 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m11:06:49.119015 [error] [MainThread]:   [TABLE_OR_VIEW_NOT_FOUND] The table or view `information_schema`.`tables` cannot be found. Verify the spelling and correctness of the schema and catalog.
[0m11:06:49.119386 [error] [MainThread]:   If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
[0m11:06:49.119643 [error] [MainThread]:   To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 12 pos 5
[0m11:06:49.119882 [info ] [MainThread]: 
[0m11:06:49.120161 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:06:49.120622 [debug] [MainThread]: Command `dbt run` failed at 11:06:49.120518 after 15.78 seconds
[0m11:06:49.120952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10559ea90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1052b2a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105213e90>]}
[0m11:06:49.121229 [debug] [MainThread]: Flushing usage events
[0m11:07:25.974519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104362ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e34ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107e35690>]}


============================== 11:07:25.976875 | a7ca9741-6d63-4757-8b69-1bc0595aabf0 ==============================
[0m11:07:25.976875 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:07:25.977135 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:07:26.695668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe4590>]}
[0m11:07:26.702578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fe4590>]}
[0m11:07:26.702811 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:07:26.716955 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:07:26.736143 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m11:07:26.736557 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/employees_renamed.sql
[0m11:07:26.747985 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:07:26.763646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1545ee2d0>]}
[0m11:07:26.766742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15477e790>]}
[0m11:07:26.766909 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:07:26.767064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105abc110>]}
[0m11:07:26.767573 [info ] [MainThread]: 
[0m11:07:26.767895 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:07:26.768336 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:07:26.768511 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:07:26.768615 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:07:26.768711 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:07:28.589130 [debug] [ThreadPool]: SQL status: OK in 1.8200000524520874 seconds
[0m11:07:28.595637 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:07:29.461457 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:07:29.463516 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:07:29.482123 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:29.482483 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:07:29.482708 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:07:29.482928 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:41.851629 [debug] [ThreadPool]: SQL status: OK in 12.369999885559082 seconds
[0m11:07:42.821928 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:07:42.823226 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:07:42.823903 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:07:42.824481 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:07:43.806194 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:07:43.816741 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:07:43.817318 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:07:43.817635 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:07:45.792289 [debug] [ThreadPool]: SQL status: OK in 1.9700000286102295 seconds
[0m11:07:45.798572 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:07:46.689147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105abc110>]}
[0m11:07:46.689997 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:46.690331 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:46.691075 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:07:46.691460 [info ] [MainThread]: 
[0m11:07:46.695949 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:07:46.696517 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:07:46.697278 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:07:46.697588 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:07:46.706402 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:07:46.711308 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:07:46.699691 => 11:07:46.711102
[0m11:07:46.711655 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:07:46.732803 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:07:46.733391 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:46.733535 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:07:46.733692 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT *
FROM (SHOW TABLES IN fivetranpocsample)

[0m11:07:46.733859 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:07:48.934256 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT *
FROM (SHOW TABLES IN fivetranpocsample)

[0m11:07:48.935506 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'IN': missing ')'. SQLSTATE: 42601 (line 9, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT *
FROM (SHOW TABLES IN fivetranpocsample)
------------------^^^

[0m11:07:48.937067 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'IN': missing ')'. SQLSTATE: 42601 (line 9, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT *
FROM (SHOW TABLES IN fivetranpocsample)
------------------^^^

	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.parser.ParseException: 
[PARSE_SYNTAX_ERROR] Syntax error at or near 'IN': missing ')'. SQLSTATE: 42601 (line 9, pos 18)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT *
FROM (SHOW TABLES IN fivetranpocsample)
------------------^^^

	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:429)
	at org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:117)
	at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:140)
	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:106)
	at com.databricks.sql.parser.DatabricksSqlParser.$anonfun$parsePlan$1(DatabricksSqlParser.scala:80)
	at com.databricks.sql.parser.DatabricksSqlParser.parse(DatabricksSqlParser.scala:101)
	at com.databricks.sql.parser.DatabricksSqlParser.parsePlan(DatabricksSqlParser.scala:77)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$3(QueryRuntimePrediction.scala:387)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:489)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$2(QueryRuntimePrediction.scala:386)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)
	at com.databricks.sql.QueryRuntimePredictionUtils$.$anonfun$getParsedPlanWithTracking$1(QueryRuntimePrediction.scala:386)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:146)
	at com.databricks.sql.QueryRuntimePredictionUtils$.getParsedPlanWithTracking(QueryRuntimePrediction.scala:385)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$2(SparkExecuteStatementOperation.scala:646)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$analyzeQuery$1(SparkExecuteStatementOperation.scala:628)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:618)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:628)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:706)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:582)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:706)
	... 42 more

[0m11:07:48.938668 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\xf5F\x16n{\xc2J4\x95\xb4\xe0\xd4\x08A\x05\xf7'
[0m11:07:48.939802 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:07:46.712065 => 11:07:48.939506
[0m11:07:48.940399 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:07:48.940880 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:07:48.941324 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:07:49.817260 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  
  [PARSE_SYNTAX_ERROR] Syntax error at or near 'IN': missing ')'. SQLSTATE: 42601 (line 9, pos 18)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
  create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
    
    
    as
      
  
  SELECT *
  FROM (SHOW TABLES IN fivetranpocsample)
  ------------------^^^
  
[0m11:07:49.817965 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7ca9741-6d63-4757-8b69-1bc0595aabf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1546a9290>]}
[0m11:07:49.818592 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 3.12s]
[0m11:07:49.819399 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:07:49.821000 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:49.821340 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:07:50.713850 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:07:50.714604 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:07:50.715101 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:07:50.715615 [debug] [MainThread]: On master: ROLLBACK
[0m11:07:50.716209 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:07:50.716631 [debug] [MainThread]: On master: Close
[0m11:07:51.586678 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:07:51.587888 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:07:51.591692 [info ] [MainThread]: 
[0m11:07:51.592587 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 24.82 seconds (24.82s).
[0m11:07:51.593572 [debug] [MainThread]: Command end result
[0m11:07:51.605525 [info ] [MainThread]: 
[0m11:07:51.606213 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:07:51.606545 [info ] [MainThread]: 
[0m11:07:51.606863 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m11:07:51.607155 [error] [MainThread]:   
[0m11:07:51.607424 [error] [MainThread]:   [PARSE_SYNTAX_ERROR] Syntax error at or near 'IN': missing ')'. SQLSTATE: 42601 (line 9, pos 18)
[0m11:07:51.607837 [error] [MainThread]:   
[0m11:07:51.608170 [error] [MainThread]:   == SQL ==
[0m11:07:51.608456 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
[0m11:07:51.608740 [error] [MainThread]:   create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
[0m11:07:51.608999 [error] [MainThread]:     
[0m11:07:51.609247 [error] [MainThread]:     
[0m11:07:51.609489 [error] [MainThread]:     as
[0m11:07:51.609726 [error] [MainThread]:       
[0m11:07:51.610130 [error] [MainThread]:   
[0m11:07:51.610366 [error] [MainThread]:   SELECT *
[0m11:07:51.610575 [error] [MainThread]:   FROM (SHOW TABLES IN fivetranpocsample)
[0m11:07:51.611029 [error] [MainThread]:   ------------------^^^
[0m11:07:51.611333 [error] [MainThread]:   
[0m11:07:51.611567 [info ] [MainThread]: 
[0m11:07:51.611835 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:07:51.612244 [debug] [MainThread]: Command `dbt run` failed at 11:07:51.612166 after 25.65 seconds
[0m11:07:51.612520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108158b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107eff890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1042b9a50>]}
[0m11:07:51.612783 [debug] [MainThread]: Flushing usage events
[0m11:10:39.129553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106846cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b67510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106617f50>]}


============================== 11:10:39.132043 | 0185e1b4-3f92-4082-b6cd-1ba25f7be879 ==============================
[0m11:10:39.132043 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:10:39.132313 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:10:39.812853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b67e90>]}
[0m11:10:39.819765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a3bb650>]}
[0m11:10:39.819988 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:10:39.833359 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:10:39.853865 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:10:39.854231 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/schema.yml
[0m11:10:39.854417 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/employees_renamed.sql
[0m11:10:39.866600 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:10:39.890195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a586390>]}
[0m11:10:39.893775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a3fe990>]}
[0m11:10:39.893996 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:10:39.894170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104224150>]}
[0m11:10:39.894813 [info ] [MainThread]: 
[0m11:10:39.895160 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:10:39.895608 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:10:39.895786 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:10:39.895897 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:10:39.895999 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:41.976333 [debug] [ThreadPool]: SQL status: OK in 2.0799999237060547 seconds
[0m11:10:41.981034 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:10:42.875281 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:10:42.875685 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:10:42.881203 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:42.881366 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:10:42.881478 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:10:42.881583 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:10:44.759173 [debug] [ThreadPool]: SQL status: OK in 1.8799999952316284 seconds
[0m11:10:44.760102 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:10:44.760367 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:10:44.760555 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:10:44.760723 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:10:45.647735 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:10:45.650625 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:10:45.650765 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:10:45.650875 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:10:47.562855 [debug] [ThreadPool]: SQL status: OK in 1.909999966621399 seconds
[0m11:10:47.568245 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:10:48.492650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a4f8ed0>]}
[0m11:10:48.493518 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:48.494053 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:10:48.495055 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:10:48.495511 [info ] [MainThread]: 
[0m11:10:48.498997 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:10:48.499277 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:10:48.499633 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:10:48.499771 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:10:48.504541 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:10:48.505026 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:10:48.499854 => 11:10:48.504927
[0m11:10:48.505175 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:10:48.517992 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:10:48.518631 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:48.518783 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:10:48.518918 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `spark_catalog`.`dbo`.`employees`

[0m11:10:48.519061 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:10:50.563059 [debug] [Thread-1 (]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    employee_id,
    first_name,
    last_name,
    email,
    hire_date,
    job_id,
    salary,
    manager_id,
    department_id
FROM `spark_catalog`.`dbo`.`employees`

[0m11:10:50.563925 [debug] [Thread-1 (]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employee_id` cannot be resolved. Did you mean one of the following? [`Department`, `EmpID`, `Salary`, `FirstName`, `HireDate`]. SQLSTATE: 42703; line 9 pos 4
[0m11:10:50.564717 [debug] [Thread-1 (]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employee_id` cannot be resolved. Did you mean one of the following? [`Department`, `EmpID`, `Salary`, `FirstName`, `HireDate`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:840)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:662)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:737)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:506)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:91)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:195)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:234)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:483)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:469)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:519)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employee_id` cannot be resolved. Did you mean one of the following? [`Department`, `EmpID`, `Salary`, `FirstName`, `HireDate`]. SQLSTATE: 42703; line 9 pos 4
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:91)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:810)
	... 42 more

[0m11:10:50.565416 [debug] [Thread-1 (]: Databricks adapter: operation-id: b'\x9d5\xc8t\x82\xcaI\xc8\xa6\xaau\xb5\xb8\xe5\x06K'
[0m11:10:50.565980 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:10:48.505259 => 11:10:50.565764
[0m11:10:50.566361 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:10:50.566681 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:10:50.566982 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:10:51.444530 [debug] [Thread-1 (]: Runtime Error in model employees_renamed (models/employees_renamed.sql)
  [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employee_id` cannot be resolved. Did you mean one of the following? [`Department`, `EmpID`, `Salary`, `FirstName`, `HireDate`]. SQLSTATE: 42703; line 9 pos 4
[0m11:10:51.444847 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0185e1b4-3f92-4082-b6cd-1ba25f7be879', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x16a4c7dd0>]}
[0m11:10:51.445157 [error] [Thread-1 (]: 1 of 1 ERROR creating sql view model fivetranpocsample.employees_renamed ....... [[31mERROR[0m in 2.95s]
[0m11:10:51.445396 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:10:51.446049 [debug] [MainThread]: On master: ROLLBACK
[0m11:10:51.446192 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:10:52.330423 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:10:52.331368 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:10:52.331843 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:10:52.332318 [debug] [MainThread]: On master: ROLLBACK
[0m11:10:52.333077 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:10:52.333424 [debug] [MainThread]: On master: Close
[0m11:10:53.225334 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:10:53.226194 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:10:53.228721 [info ] [MainThread]: 
[0m11:10:53.229493 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 13.33 seconds (13.33s).
[0m11:10:53.230175 [debug] [MainThread]: Command end result
[0m11:10:53.240184 [info ] [MainThread]: 
[0m11:10:53.240613 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:10:53.240876 [info ] [MainThread]: 
[0m11:10:53.241123 [error] [MainThread]: [33mRuntime Error in model employees_renamed (models/employees_renamed.sql)[0m
[0m11:10:53.241363 [error] [MainThread]:   [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employee_id` cannot be resolved. Did you mean one of the following? [`Department`, `EmpID`, `Salary`, `FirstName`, `HireDate`]. SQLSTATE: 42703; line 9 pos 4
[0m11:10:53.241601 [info ] [MainThread]: 
[0m11:10:53.241870 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m11:10:53.242274 [debug] [MainThread]: Command `dbt run` failed at 11:10:53.242200 after 14.13 seconds
[0m11:10:53.242549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b761d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b75f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106604650>]}
[0m11:10:53.242802 [debug] [MainThread]: Flushing usage events
[0m11:12:46.068846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067cc810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a43a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106da8190>]}


============================== 11:12:46.071338 | 7138a937-85c5-4e88-94ed-3418c50281f6 ==============================
[0m11:12:46.071338 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:12:46.071634 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:12:46.771098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac0090>]}
[0m11:12:46.778027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ac0090>]}
[0m11:12:46.778247 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:12:46.791509 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:12:46.811627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m11:12:46.811969 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/schema.yml
[0m11:12:46.812091 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/employees_renamed.sql
[0m11:12:46.823532 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:12:46.847265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a4f9fd0>]}
[0m11:12:46.850991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a685190>]}
[0m11:12:46.851158 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:12:46.851308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10445bd10>]}
[0m11:12:46.851833 [info ] [MainThread]: 
[0m11:12:46.852161 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:12:46.852591 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:12:46.852757 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:12:46.852867 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:12:46.852962 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:12:48.957135 [debug] [ThreadPool]: SQL status: OK in 2.0999999046325684 seconds
[0m11:12:48.963789 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:12:49.861562 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:12:49.863397 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:12:49.881039 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:12:49.881539 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:12:49.881816 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:12:49.882060 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:12:52.207687 [debug] [ThreadPool]: SQL status: OK in 2.3299999237060547 seconds
[0m11:12:52.209513 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:12:52.210064 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:12:52.210561 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:12:52.210957 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:12:53.092186 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:12:53.101525 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:12:53.102044 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:12:53.102799 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:12:55.200247 [debug] [ThreadPool]: SQL status: OK in 2.0999999046325684 seconds
[0m11:12:55.205524 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:12:56.172934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a5c1b10>]}
[0m11:12:56.174317 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:12:56.174803 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:12:56.175748 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:12:56.176259 [info ] [MainThread]: 
[0m11:12:56.183670 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:12:56.184556 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:12:56.185715 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:12:56.186153 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:12:56.199744 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:12:56.200577 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:12:56.186443 => 11:12:56.200384
[0m11:12:56.200888 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:12:56.221007 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:12:56.221589 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:12:56.221740 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:12:56.221917 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    EmpID as employee_id,
    FirstName as first_name,
    LastName as last_name,
    Department as department,
    Salary as salary,
    HireDate as hire_date
FROM `spark_catalog`.`dbo`.`employees`

[0m11:12:56.222119 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:12:59.700945 [debug] [Thread-1 (]: SQL status: OK in 3.4800000190734863 seconds
[0m11:12:59.718464 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:12:56.201075 => 11:12:59.718246
[0m11:12:59.718981 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:12:59.719293 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:12:59.719565 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:13:00.608591 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7138a937-85c5-4e88-94ed-3418c50281f6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x15a6977d0>]}
[0m11:13:00.609034 [info ] [Thread-1 (]: 1 of 1 OK created sql view model fivetranpocsample.employees_renamed ........... [[32mOK[0m in 4.42s]
[0m11:13:00.609361 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:13:00.610034 [debug] [MainThread]: On master: ROLLBACK
[0m11:13:00.610160 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:13:01.523873 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:13:01.524567 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:13:01.524910 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:13:01.525204 [debug] [MainThread]: On master: ROLLBACK
[0m11:13:01.525487 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:13:01.525755 [debug] [MainThread]: On master: Close
[0m11:13:02.458103 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:13:02.458556 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:13:02.460248 [info ] [MainThread]: 
[0m11:13:02.460635 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 15.61 seconds (15.61s).
[0m11:13:02.461328 [debug] [MainThread]: Command end result
[0m11:13:02.468323 [info ] [MainThread]: 
[0m11:13:02.468628 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:13:02.468852 [info ] [MainThread]: 
[0m11:13:02.469099 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:13:02.469467 [debug] [MainThread]: Command `dbt run` succeeded at 11:13:02.469402 after 16.41 seconds
[0m11:13:02.469699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aad210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106753bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067cc250>]}
[0m11:13:02.469918 [debug] [MainThread]: Flushing usage events
[0m11:26:15.915376 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044ac9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041997d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041d0090>]}


============================== 11:26:15.917775 | 030a2d0e-e23e-423f-8e6d-fe2966e0913b ==============================
[0m11:26:15.917775 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:26:15.918035 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:26:16.586529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '030a2d0e-e23e-423f-8e6d-fe2966e0913b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10452d250>]}
[0m11:26:16.593233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '030a2d0e-e23e-423f-8e6d-fe2966e0913b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10452d250>]}
[0m11:26:16.593404 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:26:16.606117 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:26:16.623742 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:26:16.623906 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:26:16.626473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '030a2d0e-e23e-423f-8e6d-fe2966e0913b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122fd2cd0>]}
[0m11:26:16.629708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '030a2d0e-e23e-423f-8e6d-fe2966e0913b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12301cc90>]}
[0m11:26:16.629899 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:26:16.630057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '030a2d0e-e23e-423f-8e6d-fe2966e0913b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101e5bfd0>]}
[0m11:26:16.630635 [debug] [MainThread]: Command `dbt ls` succeeded at 11:26:16.630583 after 0.73 seconds
[0m11:26:16.630786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104153d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044a5a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1005d6610>]}
[0m11:26:16.630901 [debug] [MainThread]: Flushing usage events
[0m11:36:46.239509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1055112d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105a24190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059c6c50>]}


============================== 11:36:46.241923 | 13245238-09b4-4403-83f7-c69fa35b4bd0 ==============================
[0m11:36:46.241923 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:36:46.242195 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:36:46.879119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13245238-09b4-4403-83f7-c69fa35b4bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059c6010>]}
[0m11:36:46.885792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13245238-09b4-4403-83f7-c69fa35b4bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f2bf450>]}
[0m11:36:46.885954 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:36:46.898901 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:36:46.918545 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:36:46.918750 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:36:46.921568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13245238-09b4-4403-83f7-c69fa35b4bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f4f1190>]}
[0m11:36:46.924706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13245238-09b4-4403-83f7-c69fa35b4bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x14f4188d0>]}
[0m11:36:46.924864 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:36:46.925026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13245238-09b4-4403-83f7-c69fa35b4bd0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10239c6d0>]}
[0m11:36:46.925625 [debug] [MainThread]: Command `dbt ls` succeeded at 11:36:46.925574 after 0.70 seconds
[0m11:36:46.925787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105513d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105512250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105993850>]}
[0m11:36:46.925900 [debug] [MainThread]: Flushing usage events
[0m11:37:23.953232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106203a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e950d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105e64090>]}


============================== 11:37:23.955590 | a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5 ==============================
[0m11:37:23.955590 [info ] [MainThread]: Running with dbt=1.5.11
[0m11:37:23.955855 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m11:37:24.578856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616a1d0>]}
[0m11:37:24.585459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1064ab450>]}
[0m11:37:24.585623 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m11:37:24.600381 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m11:37:24.600950 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m11:37:24.601164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x155b49790>]}
[0m11:37:25.004574 [debug] [MainThread]: 1699: static parser successfully parsed employees_renamed.sql
[0m11:37:25.045405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x155bfa150>]}
[0m11:37:25.048777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x155c42710>]}
[0m11:37:25.048967 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m11:37:25.049119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103aec390>]}
[0m11:37:25.049609 [info ] [MainThread]: 
[0m11:37:25.049939 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:37:25.050398 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m11:37:25.050577 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m11:37:25.050687 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m11:37:25.050788 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:37:27.451828 [debug] [ThreadPool]: SQL status: OK in 2.4000000953674316 seconds
[0m11:37:27.458918 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m11:37:28.662438 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m11:37:28.663687 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m11:37:28.677565 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:37:28.677875 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m11:37:28.678066 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m11:37:28.678224 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:32.336764 [debug] [ThreadPool]: SQL status: OK in 3.6600000858306885 seconds
[0m11:37:32.339176 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m11:37:32.339687 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:37:32.340108 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:37:32.340500 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m11:37:33.359567 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m11:37:33.368996 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:37:33.369592 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m11:37:33.369905 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:37:35.412442 [debug] [ThreadPool]: SQL status: OK in 2.0399999618530273 seconds
[0m11:37:35.425615 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m11:37:35.426166 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:37:35.426500 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "list_hive_metastore_fivetranpocsample"} */

      select current_catalog()
  
[0m11:37:36.686696 [debug] [ThreadPool]: SQL status: OK in 1.2599999904632568 seconds
[0m11:37:36.696008 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m11:37:36.696427 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "list_hive_metastore_fivetranpocsample"} */
show views in `hive_metastore`.`fivetranpocsample`
  
[0m11:37:38.108238 [debug] [ThreadPool]: SQL status: OK in 1.409999966621399 seconds
[0m11:37:38.113516 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: ROLLBACK
[0m11:37:38.114058 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m11:37:38.114404 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m11:37:39.009997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x155bf9390>]}
[0m11:37:39.011289 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:37:39.011892 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:37:39.013070 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:37:39.013756 [info ] [MainThread]: 
[0m11:37:39.021599 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_renamed
[0m11:37:39.022654 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_renamed ................ [RUN]
[0m11:37:39.023670 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_renamed)
[0m11:37:39.024059 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_renamed
[0m11:37:39.032950 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_renamed"
[0m11:37:39.034043 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (compile): 11:37:39.024830 => 11:37:39.033876
[0m11:37:39.034307 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_renamed
[0m11:37:39.053692 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_renamed"
[0m11:37:39.054235 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m11:37:39.054377 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_renamed"
[0m11:37:39.054539 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_renamed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_renamed`
  
  
  as
    

SELECT 
    EmpID as employee_id,
    FirstName as first_name,
    LastName as last_name,
    Department as department,
    Salary as salary,
    HireDate as hire_date
FROM `spark_catalog`.`dbo`.`employees`

[0m11:37:39.054717 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m11:37:43.852865 [debug] [Thread-1 (]: SQL status: OK in 4.800000190734863 seconds
[0m11:37:43.873485 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_renamed (execute): 11:37:39.034467 => 11:37:43.873252
[0m11:37:43.873982 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: ROLLBACK
[0m11:37:43.874303 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m11:37:43.874577 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_renamed: Close
[0m11:37:44.981935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a314bbd6-b6d3-4fba-973c-ed0d3f3fdda5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x155c4ee90>]}
[0m11:37:44.983646 [info ] [Thread-1 (]: 1 of 1 OK created sql view model fivetranpocsample.employees_renamed ........... [[32mOK[0m in 5.96s]
[0m11:37:44.984495 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_renamed
[0m11:37:44.986397 [debug] [MainThread]: On master: ROLLBACK
[0m11:37:44.986781 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:37:46.048755 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:37:46.050216 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m11:37:46.051059 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m11:37:46.051689 [debug] [MainThread]: On master: ROLLBACK
[0m11:37:46.052096 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m11:37:46.052468 [debug] [MainThread]: On master: Close
[0m11:37:46.930231 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:37:46.930824 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_renamed' was properly closed.
[0m11:37:46.932968 [info ] [MainThread]: 
[0m11:37:46.933520 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 21.88 seconds (21.88s).
[0m11:37:46.934150 [debug] [MainThread]: Command end result
[0m11:37:46.945145 [info ] [MainThread]: 
[0m11:37:46.945630 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:37:46.945960 [info ] [MainThread]: 
[0m11:37:46.946234 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m11:37:46.946591 [debug] [MainThread]: Command `dbt run` succeeded at 11:37:46.946524 after 23.00 seconds
[0m11:37:46.946814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106168b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10616e090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105dc5250>]}
[0m11:37:46.947040 [debug] [MainThread]: Flushing usage events
[0m15:06:59.342927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10472f5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10469d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043c5090>]}


============================== 15:06:59.345261 | dfffb9a6-2c74-4454-848a-4fc56ca3f42c ==============================
[0m15:06:59.345261 [info ] [MainThread]: Running with dbt=1.5.11
[0m15:06:59.345542 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/Users/pranayteja/.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/Users/pranayteja/fivetran_transformations/fivetran_dbt/logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True'}
[0m15:07:00.078153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043952d0>]}
[0m15:07:00.085975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043952d0>]}
[0m15:07:00.086231 [info ] [MainThread]: Registered adapter: databricks=1.5.3
[0m15:07:00.103176 [debug] [MainThread]: checksum: 007b492a958e08141e13011e4198bea48e7f359a98b57e51b7a963fca8fc83af, vars: {}, profile: , target: , version: 1.5.11
[0m15:07:00.126965 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m15:07:00.127263 [debug] [MainThread]: Partial parsing: added file: fivetran_dbt://models/employees_transformed.sql
[0m15:07:00.127474 [debug] [MainThread]: Partial parsing: updated file: fivetran_dbt://models/schema.yml
[0m15:07:00.140685 [debug] [MainThread]: 1699: static parser successfully parsed employees_transformed.sql
[0m15:07:00.161253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1377c30d0>]}
[0m15:07:00.172089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x137885910>]}
[0m15:07:00.172368 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 415 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics, 0 groups
[0m15:07:00.172535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10201c1d0>]}
[0m15:07:00.173156 [info ] [MainThread]: 
[0m15:07:00.173515 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m15:07:00.173997 [debug] [ThreadPool]: Acquiring new databricks connection 'list_hive_metastore'
[0m15:07:00.174181 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m15:07:00.174299 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=`hive_metastore`, schema=None)
[0m15:07:00.174405 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:07:03.028714 [debug] [ThreadPool]: SQL status: OK in 2.8499999046325684 seconds
[0m15:07:03.035942 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m15:07:04.051287 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_hive_metastore, now create_hive_metastore_fivetranpocsample)
[0m15:07:04.052859 [debug] [ThreadPool]: Creating schema "database: "hive_metastore"
schema: "fivetranpocsample"
"
[0m15:07:04.067246 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:07:04.067609 [debug] [ThreadPool]: Using databricks connection "create_hive_metastore_fivetranpocsample"
[0m15:07:04.067821 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "create_hive_metastore_fivetranpocsample"} */
create schema if not exists `hive_metastore`.`fivetranpocsample`
  
[0m15:07:04.068011 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:06.913691 [debug] [ThreadPool]: SQL status: OK in 2.8499999046325684 seconds
[0m15:07:06.916136 [debug] [ThreadPool]: Spark adapter: NotImplemented: commit
[0m15:07:06.916641 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: ROLLBACK
[0m15:07:06.916990 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:07:06.917277 [debug] [ThreadPool]: On create_hive_metastore_fivetranpocsample: Close
[0m15:07:07.842947 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly create_hive_metastore_fivetranpocsample, now list_hive_metastore_fivetranpocsample)
[0m15:07:07.854089 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m15:07:07.854669 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: GetTables(database=hive_metastore, schema=fivetranpocsample, identifier=None)
[0m15:07:07.855020 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:07:10.128755 [debug] [ThreadPool]: SQL status: OK in 2.2699999809265137 seconds
[0m15:07:10.143587 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m15:07:10.144144 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m15:07:10.144462 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "list_hive_metastore_fivetranpocsample"} */

      select current_catalog()
  
[0m15:07:11.284774 [debug] [ThreadPool]: SQL status: OK in 1.1399999856948853 seconds
[0m15:07:11.293821 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_fivetranpocsample"
[0m15:07:11.294232 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "connection_name": "list_hive_metastore_fivetranpocsample"} */
show views in `hive_metastore`.`fivetranpocsample`
  
[0m15:07:12.553668 [debug] [ThreadPool]: SQL status: OK in 1.2599999904632568 seconds
[0m15:07:12.559291 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: ROLLBACK
[0m15:07:12.559960 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m15:07:12.560330 [debug] [ThreadPool]: On list_hive_metastore_fivetranpocsample: Close
[0m15:07:13.485744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049d7610>]}
[0m15:07:13.487057 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:07:13.487633 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:07:13.488913 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:07:13.489774 [info ] [MainThread]: 
[0m15:07:13.496932 [debug] [Thread-1 (]: Began running node model.fivetran_dbt.employees_transformed
[0m15:07:13.498070 [info ] [Thread-1 (]: 1 of 1 START sql view model fivetranpocsample.employees_transformed ............ [RUN]
[0m15:07:13.499274 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_hive_metastore_fivetranpocsample, now model.fivetran_dbt.employees_transformed)
[0m15:07:13.499788 [debug] [Thread-1 (]: Began compiling node model.fivetran_dbt.employees_transformed
[0m15:07:13.510056 [debug] [Thread-1 (]: Writing injected SQL for node "model.fivetran_dbt.employees_transformed"
[0m15:07:13.511089 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_transformed (compile): 15:07:13.500019 => 15:07:13.510880
[0m15:07:13.511369 [debug] [Thread-1 (]: Began executing node model.fivetran_dbt.employees_transformed
[0m15:07:13.537323 [debug] [Thread-1 (]: Writing runtime sql for node "model.fivetran_dbt.employees_transformed"
[0m15:07:13.538194 [debug] [Thread-1 (]: Spark adapter: NotImplemented: add_begin_query
[0m15:07:13.538384 [debug] [Thread-1 (]: Using databricks connection "model.fivetran_dbt.employees_transformed"
[0m15:07:13.538606 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_transformed: /* {"app": "dbt", "dbt_version": "1.5.11", "dbt_databricks_version": "1.5.3", "databricks_sql_connector_version": "2.5.2", "profile_name": "fivetran_transformations", "target_name": "dev", "node_id": "model.fivetran_dbt.employees_transformed"} */
create or replace view `hive_metastore`.`fivetranpocsample`.`employees_transformed`
  
  
  as
    

SELECT 
    CAST(EmpID as bigint) as employee_id,
    UPPER(FirstName) as first_name,
    UPPER(LastName) as last_name,
    INITCAP(Department) as department,
    CAST(Salary as double) as salary,
    DATE_TRUNC('month', HireDate) as hire_date
FROM `spark_catalog`.`dbo`.`employees`

[0m15:07:13.538876 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:07:17.142477 [debug] [Thread-1 (]: SQL status: OK in 3.5999999046325684 seconds
[0m15:07:17.159008 [debug] [Thread-1 (]: Timing info for model.fivetran_dbt.employees_transformed (execute): 15:07:13.511638 => 15:07:17.158768
[0m15:07:17.159442 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_transformed: ROLLBACK
[0m15:07:17.159746 [debug] [Thread-1 (]: Databricks adapter: NotImplemented: rollback
[0m15:07:17.160013 [debug] [Thread-1 (]: On model.fivetran_dbt.employees_transformed: Close
[0m15:07:18.078646 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dfffb9a6-2c74-4454-848a-4fc56ca3f42c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1378b21d0>]}
[0m15:07:18.080706 [info ] [Thread-1 (]: 1 of 1 OK created sql view model fivetranpocsample.employees_transformed ....... [[32mOK[0m in 4.58s]
[0m15:07:18.082453 [debug] [Thread-1 (]: Finished running node model.fivetran_dbt.employees_transformed
[0m15:07:18.085610 [debug] [MainThread]: On master: ROLLBACK
[0m15:07:18.086113 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:19.209613 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:07:19.210971 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m15:07:19.211581 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m15:07:19.212178 [debug] [MainThread]: On master: ROLLBACK
[0m15:07:19.212666 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m15:07:19.213168 [debug] [MainThread]: On master: Close
[0m15:07:20.231332 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:07:20.232394 [debug] [MainThread]: Connection 'model.fivetran_dbt.employees_transformed' was properly closed.
[0m15:07:20.235845 [info ] [MainThread]: 
[0m15:07:20.236822 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 20.06 seconds (20.06s).
[0m15:07:20.238316 [debug] [MainThread]: Command end result
[0m15:07:20.251802 [info ] [MainThread]: 
[0m15:07:20.252399 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:20.252910 [info ] [MainThread]: 
[0m15:07:20.253281 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m15:07:20.253933 [debug] [MainThread]: Command `dbt run` succeeded at 15:07:20.253773 after 20.92 seconds
[0m15:07:20.254522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10446ecd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104394610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103544650>]}
[0m15:07:20.254943 [debug] [MainThread]: Flushing usage events
